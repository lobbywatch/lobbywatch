import csv
import json
import os
import re
from subprocess import call
from datetime import datetime
from shutil import copyfile
from guess_language import guess_language
from argparse import ArgumentParser
from enum import Enum, auto

import literals
import pdf_helpers
from utils import clean_whitespace, clean_str, replace_bullets

TITLE_CSV = "pg_title.csv"
DATA_CSV = "pg_data.csv"
CONTENT_PDF = "pg_file-content.pdf"

class ReadingMode(Enum):
    TITLE = auto()
    PRESIDENTS = auto()
    PRESIDENTS_SKIP_NEXT = auto()
    SEKRETARIAT = auto()
    BESCHREIBUNG = auto()
    ZWECK = auto()
    AKTIVITAETEN = auto()
    MITGLIEDER = auto()
    KONSTITUIERUNG = auto()

LABEL_MITGLIEDER = 'Mitglieder / Membres / Membri'
LABEL_KONSTITUIERUNG = 'Konstituierung / Constitution / Costituzione'
LABEL_BESCHREIBUNG = 'Beschreibung / Description / Descrizione'
LABEL_ZWECK = 'Zweck / Objectif / Scopo'
LABEL_SEKRETARIAT = 'Sekretariat / Secrétariat / Segreteria'
LABEL_PRESIDENT = 'Präsident/innen / Présidents/es / Presidenti'
LABEL_AKTIVITAETEN = 'Aktivitäten / Activités / Attività'

def extract_name_and_president_title(line):
    parts = line.split(',')
    return parts[0].strip(), parts[1].strip()

def extract_mitglied(line):
    matched_mitglied = match_mitglied(line)
    if matched_mitglied == None:
        return line.strip(), None, None, None
    name = matched_mitglied.group(1)
    rat = matched_mitglied.group(2)
    fraktion = matched_mitglied.group(4)
    kanton = matched_mitglied.group(5)
    return name, rat, fraktion, kanton

def match_mitglied(line):
    return re.search(r'(.+)\s+(NR/CN/CN|SR/CE/CS)\s+(Fraktion|Groupe)\s+([-A-Z]+)\s+([A-Z]{2})\s*', line)

def extract_sekretariat(line):
    if ':' in line and '://' not in line:
        line = line.split(':')[1]

    address_rows = re.split(r'•|\|\s+/\s+$|\s+/\s+|,|;', line)
    address_rows = [address_row.strip() for address_row in address_rows if address_row != '']
    address_rows = [address_row
                    for index, address_row
                    in enumerate(address_rows)
                    if not all(map(lambda line : re.sub(r'\d', '', line).strip() == "",
                        address_rows[index:]))
                    ]
    return address_rows


# read the csv generated by tabula and parse the groups and their members
def read_groups(filename):
    groups = []
    rows = csv.reader(open(filename, encoding="utf-8"))

    is_new_page = True
    page = 1
    reading_mode = ReadingMode.TITLE
    titles = []
    presidents = []
    sekretariat = []
    konstituierung = None
    beschreibung = []
    zweck = []
    aktivitaeten = []
    mitglieder = []
    passed_reading_modes = [reading_mode]

    lines = [clean_whitespace(clean_str(' '.join(row))) for row in rows if ''.join(row).strip() != '']
    for i, line in enumerate(lines):

        match_page = re.search(r'^(\d+)\s*/\s*\d+$', line)
        if line == '' or line.startswith('Fortsetzung:') or line.lower() in ['folgt', 'vakant', '()']:
            continue
        elif match_page:
            is_new_page = True
            new_page = match_page.group(1)
            assert page + 1 != new_page, "Page numbers not succeding, current={}, new={}".format(page, new_page)
            page = int(new_page)
            continue

        if i < len(lines) - 2:
            next_line = lines[i + 1]

        if is_new_page:
            is_new_page = False

            # save previous page
            if not match_mitglied(line) and titles and (sekretariat or konstituierung or aktivitaeten or zweck) and ReadingMode.PRESIDENTS in passed_reading_modes and ReadingMode.MITGLIEDER in passed_reading_modes and len(passed_reading_modes) > 4:
                if not presidents:
                    print("-- WARN: no presidents for group '{}'".format(titles[0]))
                if not mitglieder:
                    print("-- WARN: no mitglieder for group '{}'".format(titles[0]))
                if not sekretariat:
                    print("-- WARN: no sekretariat for group '{}'".format(titles[0]))
                duplicate_reading_modes = set([reading_mode for reading_mode in passed_reading_modes if passed_reading_modes.count(reading_mode) > 1])
                if len(duplicate_reading_modes) > 1:
                    raise Exception("ERROR: duplicate reading modes: {}".format(duplicate_reading_modes))
                if len(titles[0]) > 75 or (len(titles) > 1 and len(titles[1]) > 90) or (len(titles) > 2 and len(titles[2]) > 90) or len(titles) > 3:
                    raise Exception("ERROR: title too long:\n titles={}".format(titles))
                groups.append((titles, presidents, sekretariat, konstituierung, beschreibung, zweck, aktivitaeten, mitglieder))

                reading_mode = ReadingMode.TITLE
                titles = []
                presidents = []
                sekretariat = []
                konstituierung = None
                beschreibung = []
                zweck = []
                aktivitaeten = []
                mitglieder = []
                passed_reading_modes = [reading_mode]

        # reading_mode checks must be in reverse order as in the document
        if line.startswith(LABEL_PRESIDENT):
            reading_mode = ReadingMode.PRESIDENTS
            passed_reading_modes.append(reading_mode)

        elif line.startswith(LABEL_SEKRETARIAT):
            reading_mode = ReadingMode.SEKRETARIAT
            passed_reading_modes.append(reading_mode)

        elif line.startswith(LABEL_KONSTITUIERUNG):
            reading_mode = ReadingMode.KONSTITUIERUNG
            passed_reading_modes.append(reading_mode)

        elif line.startswith(LABEL_BESCHREIBUNG):
            reading_mode = ReadingMode.BESCHREIBUNG
            passed_reading_modes.append(reading_mode)

        elif line.startswith(LABEL_ZWECK):
            reading_mode = ReadingMode.ZWECK
            passed_reading_modes.append(reading_mode)

        elif line.startswith(LABEL_AKTIVITAETEN):
            reading_mode = ReadingMode.AKTIVITAETEN
            passed_reading_modes.append(reading_mode)

        elif line.startswith(LABEL_MITGLIEDER):
            reading_mode = ReadingMode.MITGLIEDER
            passed_reading_modes.append(reading_mode)

        elif reading_mode == ReadingMode.BESCHREIBUNG:
            text = replace_bullets(line.replace('--', ''))
            if text:
                zweck.append(text)

        elif reading_mode == ReadingMode.ZWECK:
            text = replace_bullets(line.replace('--', ''))
            if text:
                zweck.append(text)

        elif reading_mode == ReadingMode.SEKRETARIAT:
            sekretariat += extract_sekretariat(line)

        elif reading_mode == ReadingMode.PRESIDENTS:
            name, president_title = extract_name_and_president_title(line)
            presidents.append((fix_parlamentarian_name_typos(name), president_title))

        elif reading_mode == ReadingMode.MITGLIEDER:
            name, rat, fraktion, kanton = extract_mitglied(line)
            # name is one several lines
            if rat == None:
                last_mitglied = mitglieder.pop()
                name = last_mitglied[0] + " " + name
                if len(name) > 150:
                    raise Exception("ERROR: name is too long: {}".format(name))
            mitglieder.append((fix_parlamentarian_name_typos(name), None))

        elif reading_mode == ReadingMode.TITLE:
            titles.append(line)

    # save last page
    if titles and presidents:
        groups.append((titles, presidents, sekretariat, konstituierung, beschreibung, zweck, aktivitaeten, mitglieder))

    # print counts for sanity check
    print("\n{} parlamentarische Gruppen\n"
          "{} total members of parlamentarische Gruppen".format(
              len(groups),
              sum(len(gruppe) for gruppe in groups)))

    return groups

# The PDF containing the co-presidents of the parlamentarische Gruppen
# has spelling errors in certain names. Correct them here:
def fix_parlamentarian_name_typos(name):
    return name.replace("Margrit Kiener Nellen", "Margret Kiener Nellen").replace("Matthias Reynard", "Mathias Reynard").replace("Isabelle Chevallay", "Isabelle Chevalley").replace("Juerg", "Jürg").replace('Prisca Seiler Graf', 'Priska Seiler Graf').replace('Buillard-Marbach', 'Bulliard-Marbach').replace('Bulliard-Marchbach', 'Bulliard-Marbach').replace('Buillard', 'Bulliard').replace('Herzog Evy', 'Herzog Eva').replace('Levraz', 'Levrat').replace('Levart', 'Levrat').replace('Candidas', 'Candinas').replace('de Quatro', 'de Quattro').replace('Pieren Nadja', 'Umbricht Pieren Nadja').replace('Barille', 'Barrile').replace('Julliard', 'Juillard').replace('Funicello','Funiciello').replace('Franzsiska', 'Franziska').replace('Mathias Jakob Zopfi', 'Mathias Zopfi').replace('Maja Bally Frehner', 'Maya Bally')
    #.replace('Schlatter-Schmid', 'Schlatter').replace('Rüegger-Hurschler', 'Rüegger')

def normalize_namen(groups):
    new_groups = []
    for titles, members, sekretariat, konstituierung, beschreibung, zweck, art_der_aktivitaeten, mitgliederliste in groups:
        title_de = titles[0]
        title_fr = titles[1] if len(titles) > 1 else None
        title_it = titles[2] if len(titles) > 2 else None

        if (title_fr and not guess_language(title_fr, ['de', 'fr', 'it']) in ['fr', 'UNKNOWN']):
            print("Warning: title_fr '{}' guess lanuage is guessed '{}'\n".format(title_fr, guess_language(title_fr, ['de', 'fr', 'it'])))
        if (title_it and not guess_language(title_it, ['de', 'fr', 'it']) in ['it', 'UNKNOWN']):
            print("Warning: title_it '{}' guess lanuage is guessed '{}'\n".format(title_it, guess_language(title_it, ['de', 'fr', 'it'])))

        new_groups.append((clean_whitespace(title_de), clean_whitespace(title_fr), clean_whitespace(title_it), members, sekretariat, konstituierung, beschreibung, zweck, art_der_aktivitaeten, mitgliederliste))
    return new_groups

# write member of parliament and guests to json file
def write_to_json(groups, archive_pdf_name, filename, url, creation_date, modified_date, imported_date, stand_date, group_type):
    data = [{
            "name_de": title_de,
            "name_fr": title_fr,
            "name_it": title_it,
            "praesidium": members,
            "sekretariat": sekretariat,
            "konstituierung": konstituierung,
            "beschreibung": beschreibung,
            "zweck": zweck,
            "art_der_aktivitaeten": art_der_aktivitaeten,
            "mitglieder": mitgliederliste
            } for (title_de, title_fr, title_it, members, sekretariat, konstituierung, beschreibung, zweck, art_der_aktivitaeten, mitgliederliste) in groups]

    metadata_data = {
                "metadata": {
                    "archive_pdf_name": archive_pdf_name,
                    "filename": filename,
                    "url": url,
                    "pdf_creation_date": creation_date.isoformat(' '), # , timespec='minutes'
                    "pdf_modified_date": modified_date.isoformat(' '), # , timespec='minutes'
                    "imported_date": imported_date.isoformat(' '),
                    "stand_date": stand_date.isoformat(),
                    "group_type": group_type
                },
                "data": data
    }

    with open(filename, "wb") as json_file:
        contents = json.dumps(metadata_data, indent=2,
                              separators=(',', ': '),
                              ensure_ascii=False).encode("utf-8")
        json_file.write(contents)


# download a pdf containing the guest lists of members of parlament in a table
# then parse the file into json and save the json files to disk
def scrape():
    parser = ArgumentParser(description='Scarpe Parlamentarische Gruppen PDF')
    parser.add_argument("local_pdf", metavar="file", nargs='?', help="local PDF file to use", default=None)
    parser.add_argument("--group_type", metavar="TYPE", help="normal or friendship groups (TYPE=[normal,friendship])")
    args = parser.parse_args()

    if args.group_type == "friendship":
        scarpe_friendship_groups(args.local_pdf)
    else:
        scarpe_normal_groups(args.local_pdf)

def scarpe_normal_groups(local_pdf):
    url = "https://www.parlament.ch/centers/documents/de/gruppen-der-bundesversammlung.pdf"
    filename = "parlamentarische-gruppen.json"
    scarpe_parl_pdf("normal_group", url, filename, local_pdf)

def scarpe_friendship_groups(local_pdf):
    url = "https://www.parlament.ch/centers/documents/de/freundschaftsgruppe-bundesversammlung.pdf"
    filename = "parlamentarische-freundschaftsgruppen.json"
    scarpe_parl_pdf("friendship_group", url, filename, local_pdf)

def scarpe_parl_pdf(group_type, url, json_filename, local_pdf):
    print("\ntype: {}".format(group_type))
    script_path = os.path.dirname(os.path.realpath(__file__))
    try:
        import_date = datetime.now().replace(microsecond=0)
        raw_pdf_name = url.split("/")[-1]
        pdf_name = "{}-{}".format(import_date.strftime("%Y-%m-%d"), raw_pdf_name)
        if local_pdf is None:
            print("\ndownloading " + url)
            pdf_helpers.get_pdf_from_admin_ch(url, pdf_name)
        else:
            print("\ncopy local PDF " + local_pdf)
            copyfile(local_pdf, pdf_name)

        print("\nextracting metadata...")
        creation_date, modified_date = pdf_helpers.extract_creation_date(pdf_name)

        tabula_path = script_path + "/tabula-1.0.5-jar-with-dependencies.jar"
        print("parsing title page PDF...")
        cmd = ["java", "-Djava.util.logging.config.file=web_scrapers/logging.properties", "-jar", tabula_path, pdf_name, "-o", TITLE_CSV, "--pages", "1", "-t", "-i"]
        call(cmd, stderr=None)
        stand_date = pdf_helpers.read_stand(TITLE_CSV)
        archive_pdf_name = "{}-{}".format(stand_date.strftime("%Y-%m-%d"), raw_pdf_name)

        print("\nStand: {}".format(stand_date.strftime("%d.%m.%Y")))
        print("PDF creation date: {}".format(creation_date.strftime("%d.%m.%Y")))
        print("PDF modified date: {}\n".format(modified_date.strftime("%d.%m.%Y")))

        start_page = pdf_helpers.get_page_to_start_from(pdf_name)
        print(f"skipping to page {start_page} of PDF...")
        call(["qpdf", "--pages", pdf_name, f"{start_page}-z", "--", pdf_name, CONTENT_PDF])

        print("parsing PDF...")
        cmd = ["java", "-Djava.util.logging.config.file=web_scrapers/logging.properties", "-jar", tabula_path, CONTENT_PDF, "-o", DATA_CSV, "--pages", "all", "-t", "-i"]
        print(" ".join(cmd))
        call(cmd, stderr=None)

        print("reading parsed data...")
        groups = read_groups(DATA_CSV)
        groups = normalize_namen(groups)

        print("writing " + json_filename + "...")
        write_to_json(groups, archive_pdf_name, json_filename, url, creation_date, modified_date, import_date, stand_date, group_type)

        if local_pdf is None:
            archive_json_filename = "{}-{}".format(stand_date.strftime("%Y-%m-%d"), json_filename)
            print("archiving PDF " + archive_pdf_name)
            copyfile(pdf_name, script_path + "/archive/{}".format(archive_pdf_name))
            print("archiving JSON ...")
            copyfile(json_filename, script_path + "/archive/{}".format(archive_json_filename))

    finally:
        print("cleaning up...")
        if local_pdf is None:
            if os.path.isfile(pdf_name): os.rename(pdf_name, script_path + "/backup/{}".format(pdf_name))
        else:
            if os.path.isfile(pdf_name): os.remove(pdf_name)
        backup_json_filename = "{}-{}".format(import_date.strftime("%Y-%m-%d"), json_filename)
        if os.path.isfile(json_filename): copyfile(json_filename, script_path + "/backup/{}".format(backup_json_filename))
        if os.path.isfile(CONTENT_PDF): os.remove(CONTENT_PDF)
        if os.path.isfile(DATA_CSV): os.remove(DATA_CSV)
        if os.path.isfile(TITLE_CSV): os.remove(TITLE_CSV)


#main method
if __name__ == "__main__":
    scrape()
